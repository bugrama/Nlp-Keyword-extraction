{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614565e-7bba-4958-87c3-15339c786605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  GEREKLİ KÜTÜPHANELER\n",
    "\n",
    "import re, string\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "#  SABİTLER\n",
    "\n",
    "DOCS_PATH      = Path(\"docsutf8\")     # .txt dosyaları\n",
    "KEYS_PATH      = Path(\"keys\")         # .key dosyaları\n",
    "STOPWORD_PATH  = Path(\"stopwords.txt\")\n",
    "TOP_K_KEYWORDS = 10\n",
    "PUNCTUATION    = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "\n",
    "\n",
    "#  YARDIMCI FONKSİYONLAR\n",
    "\n",
    "def get_stopwords_list(path: Path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [w.strip() for w in f if w.strip()]\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_punct])\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \"\".join(c for c in text if c not in PUNCTUATION)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return lemmatize_text(text)\n",
    "\n",
    "def read_key_file(path: Path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        keys = [clean_text(line) for line in f]\n",
    "    return list({k for k in keys if k})\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    return sorted(zip(coo_matrix.col, coo_matrix.data),\n",
    "                  key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    return [feature_names[idx] for idx, _ in sorted_items]\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    sorted_items  = sort_coo(tf_idf_vector.tocoo())\n",
    "    return extract_topn_from_vector(feature_names, sorted_items, TOP_K_KEYWORDS)\n",
    "\n",
    "\n",
    "#  DÖKÜMANLARI OKU\n",
    "\n",
    "file_paths = list(DOCS_PATH.glob(\"*.txt\"))\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(f\"{DOCS_PATH} içinde .txt dosyası yok!\")\n",
    "\n",
    "corpora, basenames = [], []\n",
    "for fp in file_paths:\n",
    "    with open(fp, encoding=\"utf-8\") as f:\n",
    "        corpora.append(clean_text(f.read()))\n",
    "        basenames.append(fp.stem)\n",
    "\n",
    "\n",
    "#  TF-IDF & ANAHTAR SÖZCÜK TAHMİNİ\n",
    "\n",
    "stopwords  = get_stopwords_list(STOPWORD_PATH)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords,\n",
    "    smooth_idf=True,\n",
    "    use_idf=True,\n",
    "    ngram_range=(1, 3)          \n",
    ")\n",
    "\n",
    "vectorizer.fit_transform(corpora)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "predictions = [\n",
    "    get_keywords(vectorizer, feature_names, doc)\n",
    "    for doc in corpora\n",
    "]\n",
    "\n",
    "\n",
    "#  GERÇEK .key DOSYALARINI OKU\n",
    "\n",
    "ground_truth = []\n",
    "for base in basenames:\n",
    "    key_fp = KEYS_PATH / f\"{base}.key\"\n",
    "    if not key_fp.exists():\n",
    "        raise FileNotFoundError(f\"{key_fp} bulunamadı!\")\n",
    "    ground_truth.append(read_key_file(key_fp))    # ❷\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "for fname, pred, true in zip(basenames, predictions, ground_truth):\n",
    "    pred_set = set(pred[:10])   \n",
    "    true_set = set(true)\n",
    "    intersect = pred_set & true_set\n",
    "\n",
    "    recall_at_10 = len(intersect) / len(true_set) if true_set else 0\n",
    "    precision_at_10 = len(intersect) / 10\n",
    "\n",
    "    rows.append({\n",
    "        \"dosya\": fname,\n",
    "        \"true_keywords\": len(true_set),\n",
    "        \"doğru\": len(intersect),\n",
    "        \"precision@10\": round(precision_at_10, 3),\n",
    "        \"recall@10\": round(recall_at_10, 3),\n",
    "        \"→ tahmin\": \", \".join(pred[:10]),\n",
    "        \"→ gerçek\": \", \".join(true),\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_eval.to_csv(\"sonuc.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "display(df_eval)\n",
    "\n",
    "print(\"\\ ORTALAMA:\")\n",
    "for m in [\"precision\", \"recall\"]:\n",
    "    print(f\"  {m:12}: {df_eval[m].mean():.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
