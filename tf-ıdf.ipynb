{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614565e-7bba-4958-87c3-15339c786605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dosya</th>\n",
       "      <th>true_keywords</th>\n",
       "      <th>doğru</th>\n",
       "      <th>precision@10</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>→ tahmin</th>\n",
       "      <th>→ gerçek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C-1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.211</td>\n",
       "      <td>uddi, registry, dht, service, uddi registry, p...</td>\n",
       "      <td>bamboo dht code, uddi registry, query, dht bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C-14</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.214</td>\n",
       "      <td>sensor, exposure, deploy, minimum exposure, se...</td>\n",
       "      <td>path exposure, number of sensor, value fusion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C-17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>nmax, packet, client, css, stream, sip, audio,...</td>\n",
       "      <td>packetswitche network, audio service framework...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C-18</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.143</td>\n",
       "      <td>worm, swarm, password, swarm worm, host, zachi...</td>\n",
       "      <td>countermeasure system, internet worm, emergent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C-19</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.385</td>\n",
       "      <td>protocol module, module, protocol, service int...</td>\n",
       "      <td>dynamic protocol replacement, service interfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>J-72</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.105</td>\n",
       "      <td>valuation, elicitation, query, equivalence que...</td>\n",
       "      <td>combinatorial auction, xor bid, polynomial com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>J-73</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.143</td>\n",
       "      <td>vwap, limit order, price, sell, competitive ra...</td>\n",
       "      <td>online model, stock trading, volume weight ave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>J-74</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333</td>\n",
       "      <td>θi, biθi, auction, bid, fθi, cheat, agent, sel...</td>\n",
       "      <td>bidsecondprice auction, sealedbid, auction, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>J-8</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.087</td>\n",
       "      <td>player, connection game, strong equilibrium, g...</td>\n",
       "      <td>player number, graph topology, fair connection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>J-9</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.103</td>\n",
       "      <td>market, agent, price, round, security, equilib...</td>\n",
       "      <td>rational expectation, computational process, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dosya  true_keywords  doğru  precision@10  recall@10  \\\n",
       "0     C-1             19      4           0.4      0.211   \n",
       "1    C-14             14      3           0.3      0.214   \n",
       "2    C-17             16      1           0.1      0.062   \n",
       "3    C-18             14      2           0.2      0.143   \n",
       "4    C-19             13      5           0.5      0.385   \n",
       "..    ...            ...    ...           ...        ...   \n",
       "238  J-72             19      2           0.2      0.105   \n",
       "239  J-73             14      2           0.2      0.143   \n",
       "240  J-74             12      4           0.4      0.333   \n",
       "241   J-8             23      2           0.2      0.087   \n",
       "242   J-9             29      3           0.3      0.103   \n",
       "\n",
       "                                              → tahmin  \\\n",
       "0    uddi, registry, dht, service, uddi registry, p...   \n",
       "1    sensor, exposure, deploy, minimum exposure, se...   \n",
       "2    nmax, packet, client, css, stream, sip, audio,...   \n",
       "3    worm, swarm, password, swarm worm, host, zachi...   \n",
       "4    protocol module, module, protocol, service int...   \n",
       "..                                                 ...   \n",
       "238  valuation, elicitation, query, equivalence que...   \n",
       "239  vwap, limit order, price, sell, competitive ra...   \n",
       "240  θi, biθi, auction, bid, fθi, cheat, agent, sel...   \n",
       "241  player, connection game, strong equilibrium, g...   \n",
       "242  market, agent, price, round, security, equilib...   \n",
       "\n",
       "                                              → gerçek  \n",
       "0    bamboo dht code, uddi registry, query, dht bas...  \n",
       "1    path exposure, number of sensor, value fusion,...  \n",
       "2    packetswitche network, audio service framework...  \n",
       "3    countermeasure system, internet worm, emergent...  \n",
       "4    dynamic protocol replacement, service interfac...  \n",
       "..                                                 ...  \n",
       "238  combinatorial auction, xor bid, polynomial com...  \n",
       "239  online model, stock trading, volume weight ave...  \n",
       "240  bidsecondprice auction, sealedbid, auction, se...  \n",
       "241  player number, graph topology, fair connection...  \n",
       "242  rational expectation, computational process, t...  \n",
       "\n",
       "[243 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAKRO ORTALAMA:\n",
      "  precision@10: 0.166\n",
      "  recall@10   : 0.114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  GEREKLİ KÜTÜPHANELER\n",
    "\n",
    "import re, string\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "#  SABİTLER\n",
    "\n",
    "DOCS_PATH      = Path(\"docsutf8\")     # .txt dosyaları\n",
    "KEYS_PATH      = Path(\"keys\")         # .key dosyaları\n",
    "STOPWORD_PATH  = Path(\"stopwords.txt\")\n",
    "TOP_K_KEYWORDS = 10\n",
    "PUNCTUATION    = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "\n",
    "\n",
    "#  YARDIMCI FONKSİYONLAR\n",
    "\n",
    "def get_stopwords_list(path: Path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [w.strip() for w in f if w.strip()]\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_punct])\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = \"\".join(c for c in text if c not in PUNCTUATION)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return lemmatize_text(text)\n",
    "\n",
    "def read_key_file(path: Path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        keys = [clean_text(line) for line in f]\n",
    "    return list({k for k in keys if k})\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    return sorted(zip(coo_matrix.col, coo_matrix.data),\n",
    "                  key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    return [feature_names[idx] for idx, _ in sorted_items]\n",
    "\n",
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    sorted_items  = sort_coo(tf_idf_vector.tocoo())\n",
    "    return extract_topn_from_vector(feature_names, sorted_items, TOP_K_KEYWORDS)\n",
    "\n",
    "\n",
    "#  DÖKÜMANLARI OKU\n",
    "\n",
    "file_paths = list(DOCS_PATH.glob(\"*.txt\"))\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(f\"{DOCS_PATH} içinde .txt dosyası yok!\")\n",
    "\n",
    "corpora, basenames = [], []\n",
    "for fp in file_paths:\n",
    "    with open(fp, encoding=\"utf-8\") as f:\n",
    "        corpora.append(clean_text(f.read()))\n",
    "        basenames.append(fp.stem)\n",
    "\n",
    "\n",
    "#  TF-IDF & ANAHTAR SÖZCÜK TAHMİNİ\n",
    "\n",
    "stopwords  = get_stopwords_list(STOPWORD_PATH)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords,\n",
    "    smooth_idf=True,\n",
    "    use_idf=True,\n",
    "    ngram_range=(1, 3)          \n",
    ")\n",
    "\n",
    "vectorizer.fit_transform(corpora)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "predictions = [\n",
    "    get_keywords(vectorizer, feature_names, doc)\n",
    "    for doc in corpora\n",
    "]\n",
    "\n",
    "\n",
    "#  GERÇEK .key DOSYALARINI OKU\n",
    "\n",
    "ground_truth = []\n",
    "for base in basenames:\n",
    "    key_fp = KEYS_PATH / f\"{base}.key\"\n",
    "    if not key_fp.exists():\n",
    "        raise FileNotFoundError(f\"{key_fp} bulunamadı!\")\n",
    "    ground_truth.append(read_key_file(key_fp))    # ❷\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "for fname, pred, true in zip(basenames, predictions, ground_truth):\n",
    "    pred_set = set(pred[:10])   \n",
    "    true_set = set(true)\n",
    "    intersect = pred_set & true_set\n",
    "\n",
    "    recall_at_10 = len(intersect) / len(true_set) if true_set else 0\n",
    "    precision_at_10 = len(intersect) / 10\n",
    "\n",
    "    rows.append({\n",
    "        \"dosya\": fname,\n",
    "        \"true_keywords\": len(true_set),\n",
    "        \"doğru\": len(intersect),\n",
    "        \"precision@10\": round(precision_at_10, 3),\n",
    "        \"recall@10\": round(recall_at_10, 3),\n",
    "        \"→ tahmin\": \", \".join(pred[:10]),\n",
    "        \"→ gerçek\": \", \".join(true),\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_eval.to_csv(\"sonuc.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "display(df_eval)\n",
    "\n",
    "print(\"\\ ORTALAMA:\")\n",
    "for m in [\"precision\", \"recall\"]:\n",
    "    print(f\"  {m:12}: {df_eval[m].mean():.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
