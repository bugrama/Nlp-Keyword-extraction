{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cef6d3-8f9a-40e2-a6fa-be49cf605570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: docsutf8\\C-1.txt\n",
      "Top 10 phrases (max 3 words, highest ranked):\n",
      "UDDI registry\n",
      "query service registry\n",
      "multiple UDDI registry\n",
      "service proxy registry\n",
      "network UDDI registry\n",
      "federated UDDI service\n",
      "UDDI DHT\n",
      "similar UDDI registry\n",
      "UDDI V3 registry\n",
      "private UDDI registry\n",
      "----------------------------------------\n",
      "Processing file: docsutf8\\C-14.txt\n",
      "Top 10 phrases (max 3 words, highest ranked):\n",
      "deploy sensor\n",
      "sensor high exposure\n",
      "cost sensor\n",
      "number sensor\n",
      "deploy sensor time\n",
      "sensor\n",
      "increase number sensor\n",
      "energy sensor\n",
      "optimal number sensor\n",
      "vary number sensor\n",
      "----------------------------------------\n",
      "Processing file: docsutf8\\C-17.txt\n",
      "Top 10 phrases (max 3 words, highest ranked):\n",
      "client conference\n",
      "packet client\n",
      "domain conference\n",
      "CS conference\n",
      "stream client\n",
      "audio conference time\n",
      "NMax audio packet\n",
      "conferencing conference\n",
      "packet remote domain\n",
      "number active client\n",
      "----------------------------------------\n",
      "Processing file: docsutf8\\C-18.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Metni spaCy ile işleyerek lemmatizasyon yap\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Lemmatize edilmiş metni tekrar birleştir\u001b[39;00m\n\u001b[0;32m     25\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop])\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytextrank\\base.py:253\u001b[0m, in \u001b[0;36mBaseTextRankFactory.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    242\u001b[0m Doc\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrases\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, default\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    244\u001b[0m doc\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mtextrank \u001b[38;5;241m=\u001b[39m BaseTextRank(\n\u001b[0;32m    245\u001b[0m     doc,\n\u001b[0;32m    246\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords,\n\u001b[0;32m    251\u001b[0m     )\n\u001b[1;32m--> 253\u001b[0m doc\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mphrases \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextrank\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_textrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytextrank\\base.py:352\u001b[0m, in \u001b[0;36mBaseTextRank.calc_textrank\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlemma_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_graph()\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# to run the algorithm, we use the NetworkX implementation\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# for PageRank (i.e., based on eigenvector centrality)\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# to calculate a rank for each node in the lemma graph\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mranks \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpagerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersonalization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_personalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# agglomerate the lemmas ranked in the lemma graph into ranked\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# phrases, leveraging information from earlier stages of the\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# pipeline: noun chunks and named entities\u001b[39;00m\n\u001b[0;32m    360\u001b[0m nc_phrases: typing\u001b[38;5;241m.\u001b[39mDict[Span, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 4:3\u001b[0m, in \u001b[0;36margmap_pagerank_1\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\utils\\backends.py:967\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m backend is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[0;32m    972\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:111\u001b[0m, in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatchable(edge_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpagerank\u001b[39m(\n\u001b[0;32m     12\u001b[0m     G,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     dangling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m ):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the PageRank of the nodes in the graph.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    PageRank computes a ranking of the nodes in the graph G based on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pagerank_scipy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersonalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdangling\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:461\u001b[0m, in \u001b[0;36m_pagerank_scipy\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m    460\u001b[0m nodelist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G)\n\u001b[1;32m--> 461\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scipy_sparse_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m S \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    463\u001b[0m S[S \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m S[S \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 8:3\u001b[0m, in \u001b[0;36margmap_to_scipy_sparse_array_5\u001b[1;34m(G, nodelist, dtype, weight, format, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\utils\\backends.py:967\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m backend is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[0;32m    972\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\convert_matrix.py:687\u001b[0m, in \u001b[0;36mto_scipy_sparse_array\u001b[1;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[0;32m    684\u001b[0m         G \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msubgraph(nodelist)\n\u001b[0;32m    686\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nodelist, \u001b[38;5;28mrange\u001b[39m(nlen)))\n\u001b[1;32m--> 687\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     row, col, data \u001b[38;5;241m=\u001b[39m coefficients\n",
      "File \u001b[1;32mc:\\Users\\bugraaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\networkx\\convert_matrix.py:688\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    684\u001b[0m         G \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msubgraph(nodelist)\n\u001b[0;32m    686\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nodelist, \u001b[38;5;28mrange\u001b[39m(nlen)))\n\u001b[0;32m    687\u001b[0m coefficients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;241m*\u001b[39m((index[u], \u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m, wt) \u001b[38;5;28;01mfor\u001b[39;00m u, v, wt \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39medges(data\u001b[38;5;241m=\u001b[39mweight, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    689\u001b[0m )\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     row, col, data \u001b[38;5;241m=\u001b[39m coefficients\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter, OrderedDict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "class TextRank4KeywordGlobal():\n",
    "    def __init__(self):\n",
    "        self.d = 0.85\n",
    "        self.min_diff = 1e-5\n",
    "        self.steps = 10\n",
    "        self.node_weight = None\n",
    "\n",
    "    def set_stopwords(self, stopwords):  \n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                if token.pos_ in candidate_pos and not token.is_stop:\n",
    "                    selected_words.append(token.text.lower() if lower else token.text)\n",
    "            if selected_words:\n",
    "                sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        token_pairs = []\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i + 1, i + window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "        g = self.symmetrize(g)\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm != 0)\n",
    "        return g_norm\n",
    "\n",
    "    def analyze(self, sentences, window_size=4):\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1 - self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr)) < self.min_diff:\n",
    "                break\n",
    "            previous_pr = sum(pr)\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        self.node_weight = node_weight\n",
    "\n",
    "\n",
    "def global_textrank_local_keywords(text_folder='docsutf8', top_n=10):\n",
    "    all_sentences = []\n",
    "    file_sentences = {}\n",
    "\n",
    "    \n",
    "    for filename in sorted(os.listdir(text_folder)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(text_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                doc = nlp(text)\n",
    "                tr = TextRank4KeywordGlobal()\n",
    "                tr.set_stopwords([])\n",
    "                sents = tr.sentence_segment(doc, candidate_pos=['NOUN', 'PROPN'], lower=True)\n",
    "                file_sentences[filename] = sents\n",
    "                all_sentences.extend(sents)\n",
    "\n",
    "    \n",
    "    print(\"🔄 Global TextRank modeli oluşturuluyor...\")\n",
    "    global_tr = TextRank4KeywordGlobal()\n",
    "    global_tr.set_stopwords([])\n",
    "    global_tr.analyze(all_sentences)\n",
    "\n",
    "    \n",
    "    for filename, sents in file_sentences.items():\n",
    "        words = [word for sent in sents for word in sent]\n",
    "        local_words = set(words)\n",
    "        ranked_keywords = [(word, global_tr.node_weight[word]) for word in local_words if word in global_tr.node_weight]\n",
    "        ranked_keywords = sorted(ranked_keywords, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        print(f\"\\n📄 {filename} için anahtar kelimeler:\")\n",
    "        for word, score in ranked_keywords:\n",
    "            print(f\"  - {word} ({score:.4f})\")\n",
    "\n",
    "\n",
    "global_textrank_local_keywords('docsutf8', top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cacac3-81e6-402d-95cf-c9f33d8a9d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
