{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cef6d3-8f9a-40e2-a6fa-be49cf605570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter, OrderedDict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "class TextRank4KeywordGlobal():\n",
    "    def __init__(self):\n",
    "        self.d = 0.85\n",
    "        self.min_diff = 1e-5\n",
    "        self.steps = 10\n",
    "        self.node_weight = None\n",
    "\n",
    "    def set_stopwords(self, stopwords):  \n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                if token.pos_ in candidate_pos and not token.is_stop:\n",
    "                    selected_words.append(token.text.lower() if lower else token.text)\n",
    "            if selected_words:\n",
    "                sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        token_pairs = []\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i + 1, i + window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "        g = self.symmetrize(g)\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm != 0)\n",
    "        return g_norm\n",
    "\n",
    "    def analyze(self, sentences, window_size=4):\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1 - self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr)) < self.min_diff:\n",
    "                break\n",
    "            previous_pr = sum(pr)\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        self.node_weight = node_weight\n",
    "\n",
    "\n",
    "def global_textrank_local_keywords(text_folder='docsutf8', top_n=10):\n",
    "    all_sentences = []\n",
    "    file_sentences = {}\n",
    "\n",
    "    \n",
    "    for filename in sorted(os.listdir(text_folder)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(text_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                doc = nlp(text)\n",
    "                tr = TextRank4KeywordGlobal()\n",
    "                tr.set_stopwords([])\n",
    "                sents = tr.sentence_segment(doc, candidate_pos=['NOUN', 'PROPN'], lower=True)\n",
    "                file_sentences[filename] = sents\n",
    "                all_sentences.extend(sents)\n",
    "\n",
    "    \n",
    "    print(\" Global TextRank modeli oluşturuluyor...\")\n",
    "    global_tr = TextRank4KeywordGlobal()\n",
    "    global_tr.set_stopwords([])\n",
    "    global_tr.analyze(all_sentences)\n",
    "\n",
    "    \n",
    "    for filename, sents in file_sentences.items():\n",
    "        words = [word for sent in sents for word in sent]\n",
    "        local_words = set(words)\n",
    "        ranked_keywords = [(word, global_tr.node_weight[word]) for word in local_words if word in global_tr.node_weight]\n",
    "        ranked_keywords = sorted(ranked_keywords, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        print(f\"\\n {filename} için anahtar kelimeler:\")\n",
    "        for word, score in ranked_keywords:\n",
    "            print(f\"  - {word} ({score:.4f})\")\n",
    "\n",
    "\n",
    "global_textrank_local_keywords('docsutf8', top_n=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
