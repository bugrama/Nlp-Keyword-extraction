{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c8774-8fd8-4ebe-b932-122e9a6e5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter, OrderedDict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    def __init__(self):\n",
    "        self.d = 0.85\n",
    "        self.min_diff = 1e-5\n",
    "        self.steps = 10\n",
    "        self.node_weight = None\n",
    "\n",
    "    def set_stopwords(self, stopwords):  \n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                if token.pos_ in candidate_pos and not token.is_stop:\n",
    "                    selected_words.append(token.text.lower() if lower else token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        token_pairs = []\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i + 1, i + window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "        g = self.symmetrize(g)\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0)\n",
    "        return g_norm\n",
    "\n",
    "    def analyze(self, text, candidate_pos=['NOUN', 'PROPN'], window_size=4, lower=True, stopwords=[]):\n",
    "        self.set_stopwords(stopwords)\n",
    "        doc = nlp(text)\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower)\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1 - self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr)) < self.min_diff:\n",
    "                break\n",
    "            previous_pr = sum(pr)\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        self.node_weight = node_weight\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "def load_ground_truth_keywords(key_path):\n",
    "    keywords = []\n",
    "    with open(key_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            keywords.append(line.strip().lower())\n",
    "    return keywords\n",
    "\n",
    "def evaluate_keywords(predicted, ground_truth):\n",
    "    predicted_set = set(predicted)\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    true_positive = predicted_set.intersection(ground_truth_set)\n",
    "    precision = len(true_positive) / len(predicted_set) if predicted_set else 0\n",
    "    recall = len(true_positive) / len(ground_truth_set) if ground_truth_set else 0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_all_files(text_folder='docsutf8', key_folder='keys', top_n=10):\n",
    "    scores = []\n",
    "    first_file_done = False\n",
    "\n",
    "    for filename in sorted(os.listdir(text_folder)):\n",
    "        if filename.endswith('.txt'):\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            text_path = os.path.join(text_folder, filename)\n",
    "            key_path = os.path.join(key_folder, base_name + '.key')\n",
    "\n",
    "            \n",
    "\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            tr4w = TextRank4Keyword()\n",
    "            tr4w.analyze(text)\n",
    "\n",
    "            keywords_1gram = list(tr4w.node_weight.keys())\n",
    "            keywords_2gram = generate_ngrams(keywords_1gram, 2)\n",
    "            keywords_3gram = generate_ngrams(keywords_1gram, 3)\n",
    "\n",
    "            all_keywords = Counter()\n",
    "            for kw in keywords_1gram:\n",
    "                all_keywords[kw] += tr4w.node_weight.get(kw, 1)\n",
    "            for kw in keywords_2gram:\n",
    "                all_keywords[kw] += sum([tr4w.node_weight.get(w, 1) for w in kw.split()]) / 2\n",
    "            for kw in keywords_3gram:\n",
    "                all_keywords[kw] += sum([tr4w.node_weight.get(w, 1) for w in kw.split()]) / 3\n",
    "\n",
    "            predicted_keywords = [kw for kw, _ in all_keywords.most_common(top_n)]\n",
    "            ground_truth = load_ground_truth_keywords(key_path)\n",
    "\n",
    "           \n",
    "            if not first_file_done:\n",
    "                print(f\"\\n ƒ∞lk dosya: {filename}\")\n",
    "                print(\"\\n Tahmin Edilen Anahtar Kelimeler:\")\n",
    "                for kw in predicted_keywords:\n",
    "                    print(f\"  - {kw}\")\n",
    "                print(\"\\n Ger√ßek Anahtar Kelimeler:\")\n",
    "                for kw in ground_truth:\n",
    "                    print(f\"  - {kw}\")\n",
    "                first_file_done = True\n",
    "\n",
    "            precision, recall, f1 = evaluate_keywords(predicted_keywords, ground_truth)\n",
    "            scores.append((precision, recall, f1))\n",
    "            print(f\"\\nüìÑ {filename} -> Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "\n",
    "    if scores:\n",
    "        avg_precision = sum([s[0] for s in scores]) / len(scores)\n",
    "        avg_recall = sum([s[1] for s in scores]) / len(scores)\n",
    "        avg_f1 = sum([s[2] for s in scores]) / len(scores)\n",
    "        print(f\"\\n Ortalama Sonu√ßlar -> Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1: {avg_f1:.2f}\")\n",
    "    else:\n",
    "        print(\"Hi√ß uygun e≈üle≈üme bulunamadƒ±.\")\n",
    "\n",
    "\n",
    "evaluate_all_files('docsutf8', 'keys', top_n=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
